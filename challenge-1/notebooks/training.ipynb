{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOeqDT4-p5J3"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Author: Annam.ai IIT Ropar\n",
        "Team Name: SoilClassifiers\n",
        "Team Members: Krishnopreya , Debapriya , Shweta, Namya, Nikhil\n",
        "Leaderboard Rank:101\n",
        "\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Add src directory to path\n",
        "sys.path.append('src')\n",
        "\n",
        "from preprocessing import load_and_split_data, prepare_datasets\n",
        "from training import train_model, SoilClassifier\n",
        "from postprocessing import process_results\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "\n",
        "    print(\"Starting Soil Classification Training Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Check device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Define paths\n",
        "    DATA_DIR = \"/kaggle/input/soil-classification/soil_classification-2025\"\n",
        "    TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
        "    TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
        "    TRAIN_CSV = os.path.join(DATA_DIR, \"train_labels.csv\")\n",
        "    TEST_CSV = os.path.join(DATA_DIR, \"test_ids.csv\")\n",
        "\n",
        "    # Output directory\n",
        "    OUTPUT_DIR = \"/kaggle/working\"\n",
        "    MODEL_PATH = os.path.join(OUTPUT_DIR, \"best_model.pth\")\n",
        "\n",
        "    print(f\"Data directory: {DATA_DIR}\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "    # Step 1: Load and split data\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 1: Loading and splitting data\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    train_df, val_df = load_and_split_data(TRAIN_CSV)\n",
        "    print(f\"Training samples: {len(train_df)}\")\n",
        "    print(f\"Validation samples: {len(val_df)}\")\n",
        "\n",
        "    # Step 2: Prepare datasets\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 2: Preparing datasets\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset, test_df = prepare_datasets(\n",
        "        train_df, val_df, TEST_CSV, TRAIN_DIR, TEST_DIR\n",
        "    )\n",
        "\n",
        "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    # Step 3: Train model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 3: Training model\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model, history = train_model(\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        device,\n",
        "        MODEL_PATH,\n",
        "        num_epochs=20,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    # Step 4: Post-processing and evaluation\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 4: Post-processing and evaluation\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(MODEL_PATH))\n",
        "    model.eval()\n",
        "\n",
        "    # Process results\n",
        "    submission_df = process_results(\n",
        "        model, val_dataset, test_dataset, test_df, history, device, OUTPUT_DIR\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\nFiles generated:\")\n",
        "    print(f\"- Model: {MODEL_PATH}\")\n",
        "    print(f\"- Submission: {os.path.join(OUTPUT_DIR, 'submission.csv')}\")\n",
        "    print(f\"- Confusion Matrix: {os.path.join(OUTPUT_DIR, 'confusion_matrix.png')}\")\n",
        "    print(f\"- Metrics: {os.path.join(OUTPUT_DIR, 'ml-metrics.json')}\")\n",
        "    print(f\"- Training History: {os.path.join(OUTPUT_DIR, 'training_history.png')}\")\n",
        "    print(f\"- Architecture Info: {os.path.join(OUTPUT_DIR, 'architecture.json')}\")\n",
        "\n",
        "    print(f\"\\nFinal validation accuracy: {max(history['val_acc']):.2f}%\")\n",
        "    print(f\"Submission shape: {submission_df.shape}\")\n",
        "    print(\"\\nPrediction distribution:\")\n",
        "    print(submission_df['soil_type'].value_counts())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}
